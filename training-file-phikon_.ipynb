{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18647,"databundleVersionId":1126921,"sourceType":"competition"},{"sourceId":11969229,"sourceType":"datasetVersion","datasetId":7526534},{"sourceId":11982406,"sourceType":"datasetVersion","datasetId":7536036},{"sourceId":12009978,"sourceType":"datasetVersion","datasetId":7555735}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Pip install pytorch lightning","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-lightning --quiet\n!pip install warmup-scheduler --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import ","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import StratifiedKFold\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom warmup_scheduler import GradualWarmupScheduler\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom torchmetrics.classification import Accuracy\nfrom pathlib import Path\nimport pandas as pd\nimport torchmetrics\nfrom sklearn.metrics import cohen_kappa_score\nimport shutil\nimport zipfile\nimport numpy as np\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating a configuration class","metadata":{}},{"cell_type":"code","source":"class Config:\n    def __init__(self):\n        self.input_dim = 768\n        self.hidden_dim = 512\n        self.num_classes = 5  # âžœ 0-4 for ordinal/BCE approach\n        self.lr = 3e-4\n        self.batch_size = 32\n        self.max_epochs = 50  \n        self.num_workers = 4\n        self.n_splits = 5\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.checkpoint_dir = \"/kaggle/working/checkpoints\"\n        self.logs_dir = \"/kaggle/working/logs\"\n        self.train_csv = \"/kaggle/input/prostate-cancer-grade-assessment/train.csv\"\n        self.slide_vectors_path = '/kaggle/input/complete-slide-feature-vectors'\n        self.noise_gap_thresh = 1.6\n\n        # criterion and scheduler configs\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.warmup_factor = 10\n        self.warmup_epo = 1\n        \ncfg = Config()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading all slide vectors and slide labels","metadata":{}},{"cell_type":"code","source":"\n# load in all .pt files from the slide vector directory\n# all slide embeddings are generated by the extractor.embedding.ipynb\n# slide embeddings createde by phikon + titan\ndef load_all_slide_vectors_from_dir(directory):\n    directory = Path(directory)\n    pt_files = list(directory.glob(\"*.pt\"))\n    print(f\"Found {len(pt_files)} .pt files in {directory}\")\n    slide_vectors = {}\n    for file_path in pt_files:\n        chunk = torch.load(str(file_path), map_location=\"cpu\")\n        slide_vectors.update(chunk)\n    print(f\"Loaded {len(slide_vectors)} slide vectors in total.\")\n    return slide_vectors\n\n# load all slides id names\ndef load_slide_labels(csv_path):\n    df = pd.read_csv(csv_path)\n    df = df[[\"image_id\", \"isup_grade\"]].rename(columns={\"image_id\": \"slide_id\", \"isup_grade\": \"label\"})\n    slide_labels = dict(zip(df[\"slide_id\"], df[\"label\"]))\n    print(f\"Loaded {len(slide_labels)} slide labels from {csv_path}\")\n    return slide_labels\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Removing slides to prevent label denoising","metadata":{}},{"cell_type":"code","source":"# get the out of fold predictions, and use a treshold of 1.6 to discard all noisy labels\ndef remove_noisy_labels(df, thresh=1.6):\n    gap = np.abs(df[\"label\"] - df[\"probs_raw\"])\n    df_keep = df[gap <= thresh].reset_index(drop=True)\n    df_removed = df[gap > thresh].reset_index(drop=True)\n    print(f\"Removed {len(df_removed)} noisy labels (threshold={thresh})\")\n    return df_keep, df_removed","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"slide_vectors = load_all_slide_vectors_from_dir(cfg.slide_vectors_path)\nslide_labels = load_slide_labels(cfg.train_csv)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating a custom slide dataset","metadata":{}},{"cell_type":"code","source":"class SlideDataset(Dataset):\n    def __init__(self, slide_vectors, slide_labels, ordinal=True):\n        self.slide_ids = list(slide_vectors.keys())\n        self.slide_vectors = slide_vectors\n        self.slide_labels = slide_labels\n        self.ordinal = ordinal\n\n    def __len__(self):  \n        return len(self.slide_ids)\n\n    def __getitem__(self, idx):\n        slide_id = self.slide_ids[idx]\n        vector = self.slide_vectors[slide_id]\n        label = self.slide_labels[slide_id]\n\n        if self.ordinal:\n            # convert integer label (0-5) to ordinal binary format\n            #for instance label=3 -> [1,1,1,0,0]\n            ordinal_label = torch.zeros(5, dtype=torch.float32)\n            ordinal_label[:label] = 1\n            return vector, ordinal_label\n        else:\n            return vector, label\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = SlideDataset(slide_vectors, slide_labels)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"code","source":"# Get one batch to inspect\nfor batch_vectors, batch_labels in dataloader:\n    print(\"Batch vectors shape:\", batch_vectors.shape)\n    print(\"Batch labels shape:\", batch_labels.shape)\n    print(\"Batch labels:\", batch_labels)\n    break  # only print the first batch!\n\nprint(\"Dataset length:\", len(dataset))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a lightning slide classifier","metadata":{}},{"cell_type":"code","source":"# create a slide classifier using the pytorch lightning module:\n# - incorporates kaiming initialization\n# - a 4 layer MLP\n# - Cosine Annealing scheduler, with GradualWarmupScheduler, and early stopping\n# - makes use of BCEwithlogitloss,  and thus ordinal labelling\nclass SlideClassifier(pl.LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.save_hyperparameters(cfg.__dict__)\n        self.head = nn.Sequential(\n            nn.Linear(cfg.input_dim, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128),\n            nn.Dropout(0.3),\n            nn.Linear(128, cfg.num_classes)\n        )\n        self.criterion = nn.BCEWithLogitsLoss()\n        self.val_preds = []\n        self.val_targets = []\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.kaiming_normal_(m.weight)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        return self.head(x)\n\n    def training_step(self, batch, batch_idx):\n        vectors, labels = batch\n        logits = self(vectors)\n        loss = self.criterion(logits, labels)\n        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        vectors, labels = batch\n        logits = self(vectors)\n        loss = self.criterion(logits, labels)\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n        preds = (torch.sigmoid(logits) > 0.5).float().sum(1).detach().cpu().numpy()\n        targets = labels.sum(1).detach().cpu().numpy()\n        self.val_preds.extend(preds)\n        self.val_targets.extend(targets)\n\n    def on_validation_epoch_end(self):\n        if self.val_preds and self.val_targets:\n            qwk = cohen_kappa_score(self.val_preds, self.val_targets, weights=\"quadratic\")\n            acc = (np.array(self.val_preds) == np.array(self.val_targets)).mean()\n            self.log(\"val_qwk\", qwk, prog_bar=True)\n            self.log(\"val_acc_isup\", acc, prog_bar=True)\n        self.val_preds.clear()\n        self.val_targets.clear()\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams[\"lr\"] / self.hparams[\"warmup_factor\"])\n        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, T_max=max(1, self.hparams[\"max_epochs\"] - self.hparams[\"warmup_epo\"]))\n        scheduler = GradualWarmupScheduler(\n            optimizer, multiplier=self.hparams[\"warmup_factor\"],\n            total_epoch=self.hparams[\"warmup_epo\"], after_scheduler=scheduler_cosine)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"epoch\"}}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create a lightning training stratified k-fold loop","metadata":{}},{"cell_type":"code","source":"# creates a 5 k-fold training loop\n# saves checkpoints based on val_qwk score\n# apply early stopping to prevent overfitting\ndef train_kfold(cfg, slide_vectors, slide_labels):\n    dataset = SlideDataset(slide_vectors, slide_labels, ordinal=True)\n    labels_list = [slide_labels[slide_id] for slide_id in dataset.slide_ids]\n    skf = StratifiedKFold(n_splits=cfg.n_splits, shuffle=True, random_state=42)\n    best_model_paths = []\n    for fold, (train_idx, val_idx) in enumerate(skf.split(dataset.slide_ids, labels_list)):\n        print(f\"\\n Fold {fold+1}/{cfg.n_splits}\")\n        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n        val_loader = DataLoader(Subset(dataset, val_idx), batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n        checkpoint = ModelCheckpoint(dirpath=f\"{cfg.checkpoint_dir}/fold{fold+1}\", filename=\"best\", save_top_k=1, monitor=\"val_qwk\", mode=\"max\")\n        early_stop = EarlyStopping(monitor=\"val_qwk\", min_delta=0.001, patience=10, mode=\"max\")\n        logger = TensorBoardLogger(save_dir=cfg.logs_dir, name=f\"fold{fold+1}\")\n        model = SlideClassifier(cfg)\n        trainer = pl.Trainer(max_epochs=cfg.max_epochs, accelerator=cfg.device,\n                             callbacks=[checkpoint, early_stop], logger=logger, log_every_n_steps=10)\n        trainer.fit(model, train_loader, val_loader)\n        best_model_paths.append(checkpoint.best_model_path)\n    print(\"\\n All folds done!\")\n    return best_model_paths","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## outfold predictions\n\n","metadata":{}},{"cell_type":"code","source":"def predict_training_data(cfg, slide_vectors, slide_labels, model_paths):\n    dataset = SlideDataset(slide_vectors, slide_labels, ordinal=True)\n    loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n    all_probs = []\n    for model_path in model_paths:\n        model = SlideClassifier.load_from_checkpoint(model_path, cfg=cfg)\n        model.eval()\n        model.to(cfg.device)\n        preds = []\n        with torch.no_grad():\n            for vectors, _ in loader:\n                vectors = vectors.to(cfg.device)\n                logits = model(vectors)\n                # probs = torch.sigmoid(logits).mean(1).cpu().numpy()\n                probs = torch.sigmoid(logits).sum(1).cpu().numpy()\n                preds.extend(probs)\n        all_probs.append(preds)\n    mean_probs = np.mean(all_probs, axis=0)\n    return dict(zip(dataset.slide_ids, mean_probs))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main loop to get everythinng in one loop","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    slide_vectors = load_all_slide_vectors_from_dir(cfg.slide_vectors_path)\n    slide_labels = load_slide_labels(cfg.train_csv)\n\n    print(\"\\n STEP 1: Initial training (no cleaning)\")\n    first_model_paths = train_kfold(cfg, slide_vectors, slide_labels)\n\n    print(\"\\n STEP 2: Predict training data\")\n    slide_probs = predict_training_data(cfg, slide_vectors, slide_labels, first_model_paths)\n\n    # remove noisy labels\n    raw_labels_df = pd.read_csv(cfg.train_csv).rename(columns={\"image_id\": \"slide_id\", \"isup_grade\": \"label\"})\n    raw_labels_df[\"probs_raw\"] = raw_labels_df[\"slide_id\"].map(slide_probs)\n    cleaned_df, removed_df = remove_noisy_labels(raw_labels_df, thresh=cfg.noise_gap_thresh)\n    cleaned_slide_labels = dict(zip(cleaned_df[\"slide_id\"], cleaned_df[\"label\"]))\n    cleaned_slide_vectors = {sid: slide_vectors[sid] for sid in cleaned_df[\"slide_id\"] if sid in slide_vectors}\n    print(f\"\\n STEP 3: {len(cleaned_slide_vectors)} samples retained after noise removal.\")\n\n    print(\"\\n STEP 4: Retraining final model on cleaned data\")\n    best_model_paths = train_kfold(cfg, cleaned_slide_vectors, cleaned_slide_labels)\n\n    print(\"\\n Final best model checkpoints:\")\n    for path in best_model_paths:\n        print(path)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T17:43:39.434Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## To download best models\n","metadata":{}},{"cell_type":"code","source":"\ndef get_best_models(checkpoints_dir=\"/kaggle/working/checkpoints\",\n                                output_dir=\"/kaggle/working/best_models/5fold/labeldenoising\",\n                                output_zip=\"/kaggle/working/best_models.zip\"):\n    # create output dir to put in zip file of best models\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # loop through each fold directory\n    for fold_dir in Path(checkpoints_dir).iterdir():\n        if fold_dir.is_dir():\n            # Check for .ckpt file inside\n            ckpt_files = list(fold_dir.glob(\"*.ckpt\"))\n            if ckpt_files:\n                # just take the first\n                ckpt_file = ckpt_files[0]\n                # rename and copy to output_dir\n                new_name = f\"{fold_dir.name}_best_classifier.ckpt\"\n                shutil.copy(ckpt_file, os.path.join(output_dir, new_name))\n    # Zip the folder\n    shutil.make_archive(output_dir, 'zip', output_dir)\n    print(f\" Zipped folder: {output_zip}\")\n\n# Call the function\nget_best_models()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T17:43:39.435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}