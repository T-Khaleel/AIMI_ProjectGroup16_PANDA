{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":18647,"databundleVersionId":1126921,"isSourceIdPinned":false,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PANDA Slide Vector Extraction Pipeline using Trident + Titan\n## Step 1: Deduplicate slides using image hashing\n## Step 2: Remove penmarked slides\n## Step 3: Extract slide vectors (tissue segmentation, patch encoding, MIL aggregation)","metadata":{}},{"cell_type":"markdown","source":"# Install all dependencies","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/mahmoodlab/TRIDENT.git --quiet\n!apt-get install -y openslide-tools > /dev/null\n!pip install openslide-python imagehash --quiet\n!pip install huggingface_hub --quiet\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T11:50:28.707906Z","iopub.execute_input":"2025-05-26T11:50:28.708711Z","iopub.status.idle":"2025-05-26T11:51:58.957466Z","shell.execute_reply.started":"2025-05-26T11:50:28.708684Z","shell.execute_reply":"2025-05-26T11:51:58.956655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(\"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\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T11:53:21.904380Z","iopub.execute_input":"2025-05-26T11:53:21.905186Z","iopub.status.idle":"2025-05-26T11:53:22.027087Z","shell.execute_reply.started":"2025-05-26T11:53:21.905155Z","shell.execute_reply":"2025-05-26T11:53:22.026532Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import all dependencies","metadata":{}},{"cell_type":"code","source":"# Import all statements\nimport time\nimport os\nimport json\nimport torch\nimport h5py\nfrom pathlib import Path\nfrom PIL import Image\nimport numpy as np\nimport imagehash\nfrom openslide import OpenSlide\nfrom trident import load_wsi\nfrom trident.segmentation_models import segmentation_model_factory\nfrom trident.patch_encoder_models import encoder_factory as patch_factory\nfrom trident.slide_encoder_models import encoder_factory as slide_factory\nimport pickle\nimport geopandas as gpd\nimport shutil\n\n# Monkey-patch for compatibility\nif not hasattr(gpd.GeoSeries, \"union_all\"):\n    gpd.GeoSeries.union_all = lambda self, *args, **kwargs: self.unary_union\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T11:53:23.800627Z","iopub.execute_input":"2025-05-26T11:53:23.801285Z","iopub.status.idle":"2025-05-26T11:53:33.796479Z","shell.execute_reply.started":"2025-05-26T11:53:23.801263Z","shell.execute_reply":"2025-05-26T11:53:33.795722Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"class Config:\n    def __init__(self,\n                 input_dir=\"/kaggle/input/prostate-cancer-grade-assessment/train_images\",\n                 output_dir=\"/kaggle/working/slide_vectors\",\n                 clean_slide_list=\"/kaggle/working/clean_slides.json\",\n                 do_deduplication=True,\n                 do_penmark_check=True,\n                 preprocessing = False,\n                 hash_threshold=0.9,\n                 patch_mag=10,\n                 patch_size=224,\n                 patch_encoder_name=\"phikon\",\n                 slide_encoder_name=\"titan\",\n                 device=None):\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.clean_slide_list = clean_slide_list\n        self.do_deduplication = do_deduplication\n        self.preprocessing = preprocessing\n        self.do_penmark_check = do_penmark_check\n        self.hash_threshold = hash_threshold\n        self.patch_mag = patch_mag\n        self.patch_size = patch_size\n        self.patch_encoder_name = patch_encoder_name\n        self.slide_encoder_name = slide_encoder_name\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T11:53:33.797606Z","iopub.execute_input":"2025-05-26T11:53:33.797965Z","iopub.status.idle":"2025-05-26T11:53:33.803400Z","shell.execute_reply.started":"2025-05-26T11:53:33.797947Z","shell.execute_reply":"2025-05-26T11:53:33.802702Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Removing penmarks","metadata":{}},{"cell_type":"code","source":"def has_penmarks(slide_path):\n    try:\n        slide = OpenSlide(str(slide_path))\n        img = slide.read_region((0, 0), 2, (256, 256)).convert(\"RGB\")\n        arr = np.array(img)\n        pen_pixels = np.sum((arr[:, :, 2] > 150) & (arr[:, :, 0] < 100))  # Bright blue\n        return pen_pixels > 500\n    except:\n        return True  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T11:53:38.596070Z","iopub.execute_input":"2025-05-26T11:53:38.596341Z","iopub.status.idle":"2025-05-26T11:53:38.601043Z","shell.execute_reply.started":"2025-05-26T11:53:38.596314Z","shell.execute_reply":"2025-05-26T11:53:38.600368Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing by image hash and removing penmarks","metadata":{}},{"cell_type":"code","source":"def preprocess_slides(cfg):\n    print(\"Start Preprocessing ...\")\n    slide_paths = list(Path(cfg.input_dir).glob(\"*.tiff\"))\n    print(f\"found {len(slide_paths)} slides\")\n\n    # step 1: found duplicates using imagehash\n    hashes = {}\n    unique_slides = []\n    start_total = time.time()\n    \n    if cfg.do_deduplication:\n        for slide in slide_paths:  \n            try:\n                img = Image.open(slide).resize((256, 256))\n                h = imagehash.average_hash(img)\n                if not any(h - v < (1 - cfg.hash_threshold) * len(h.hash) ** 2 for v in hashes.values()):\n                    hashes[slide.name] = h\n                    unique_slides.append(slide)\n            except Exception as e:\n                print(f\" Skipping {slide.name}: {e}\")\n        print(f\"After deduplication: {len(unique_slides)}\")\n        slide_paths = unique_slides  # replace original list with deduplicated slides\n\n    # step 2: removing penmark slides\n    if cfg.do_penmark_check:\n        clean_slides = [s.name for s in slide_paths if not has_penmarks(s)]\n    else:\n        clean_slides = [s.name for s in slide_paths]\n    print(f\"After Penmark removal: {len(clean_slides)} slides \")\n\n    # Step 3: save all clean slides in a json file\n    with open(cfg.clean_slide_list, \"w\") as f:\n        json.dump(clean_slides, f)\n    print(f\" Saved clean slide list to {cfg.clean_slide_list}\")\n\n    # Timing summary\n    end_total = time.time()\n    total_time = end_total - start_total\n    total_checked = len(slide_paths)\n    avg_time = total_time / total_checked if total_checked else 0\n\n    print(f\"\\n Total preprocessing time (dedup + penmark): {total_time:.2f} seconds\")\n    print(f\" Average time per slide: {avg_time:.2f} seconds\")\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T11:53:39.003172Z","iopub.execute_input":"2025-05-26T11:53:39.003385Z","iopub.status.idle":"2025-05-26T11:53:39.011091Z","shell.execute_reply.started":"2025-05-26T11:53:39.003370Z","shell.execute_reply":"2025-05-26T11:53:39.010423Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extracting features","metadata":{}},{"cell_type":"code","source":"def extract_features(cfg):\n    print(\"Starting slide vector extraction...\")\n\n    # Step 1: Determine which slide paths to use\n    if cfg.preprocessing:\n        with open(cfg.clean_slide_list, \"r\") as f:\n            slide_names = json.load(f)\n        slide_paths = [Path(cfg.input_dir) / name for name in slide_names]\n    else:\n        slide_paths = list(Path(cfg.input_dir).glob(\"*.tiff\"))\n\n    slide_paths.sort(key=lambda x: x.name)\n\n    # step 2 : create a saving directory, and mkdir because otherwise it will crash since it doesn't exist the first time\n    save_dir = Path(cfg.output_dir) / \"train\"\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # Step 3: Load Trident models\n    seg = segmentation_model_factory(\"hest\", confidence_thresh=0.5)\n    patch_encoder = patch_factory(cfg.patch_encoder_name).eval().to(cfg.device)\n    slide_encoder = slide_factory(cfg.slide_encoder_name).eval().to(cfg.device)\n\n    # Step 4: Create dedicated folder for combined .pt files, using encoder combo in name\n    encoder_combo = f\"{cfg.patch_encoder_name}_{cfg.slide_encoder_name}\"\n    combined_vectors_dir = Path(cfg.output_dir) / \"combined\"\n    combined_vectors_dir.mkdir(parents=True, exist_ok=True)\n    \n\n    # Step 5: Loop over slides\n    slide_vectors = {}\n    slide_times = []\n\n    # start_idx = 5000  # Skip first 5000 slides\n    # for idx, slide_path in enumerate(slide_paths[start_idx:], start_idx + 1):\n\n    start_idx = 9000\n    end_idx = len(slide_paths)\n    \n    for idx, slide_path in enumerate(slide_paths[start_idx:end_idx], start_idx + 1):\n        out_path = save_dir / f\"{slide_path.stem}_{encoder_combo}.pt\"\n        if out_path.exists():\n            print(f\" Skipping {slide_path.name}, already processed.\")\n            continue\n\n        try:\n            start_time = time.time()\n            job_dir = save_dir / slide_path.stem\n            job_dir.mkdir(parents=True, exist_ok=True)\n\n            print(f\" Processing {slide_path.name}...\")\n            wsi = load_wsi(slide_path, lazy_init=False)\n\n            # Tissue segmentation\n            wsi.segment_tissue(seg, seg.target_mag, job_dir, cfg.device)\n\n            # Extract patch coordinates\n            coords_path = wsi.extract_tissue_coords(\n                target_mag=cfg.patch_mag,\n                patch_size=cfg.patch_size,\n                save_coords=str(job_dir),\n            )\n\n            # Extract patch features (saved to job_dir/patches/)\n            patch_features_path = wsi.extract_patch_features(\n                patch_encoder=patch_encoder,\n                coords_path=str(coords_path),\n                save_features=str(job_dir),\n                device=cfg.device,\n                batch_limit=32,\n            )\n\n            # create embeddings directory and extract slide-level features\n            embeddings_dir = job_dir / \"embeddings\"\n            embeddings_dir.mkdir(parents=True, exist_ok=True)\n\n            slide_vector_path = wsi.extract_slide_features(\n                str(patch_features_path),\n                slide_encoder,\n                str(embeddings_dir),\n                cfg.device\n            )\n\n            # convert the h5 file into pt for using pytorch\n            embeddings_h5 = embeddings_dir / f\"{slide_path.stem}.h5\"\n            if embeddings_h5.exists():\n                with h5py.File(embeddings_h5, \"r\") as f:\n                    features = f[\"features\"][:]\n                    tensor = torch.from_numpy(features)\n                    slide_vectors[slide_path.stem] = tensor\n                    torch.save(tensor, out_path)\n            else:\n                print(f\" Warning: embeddings .h5 file not found for {slide_path.stem}\")\n\n            # save metadata\n            with open(job_dir / \"meta.json\", \"w\") as f:\n                json.dump({\n                    \"slide_name\": slide_path.name,\n                    \"patch_encoder\": cfg.patch_encoder_name,\n                    \"slide_encoder\": cfg.slide_encoder_name,\n                    \"patch_mag\": cfg.patch_mag,\n                    \"patch_size\": cfg.patch_size\n                }, f)\n\n            elapsed = time.time() - start_time\n            slide_times.append(elapsed)\n            print(f\" Done: {slide_path.name} in {elapsed:.2f} seconds\")\n\n            # average every 10 slides\n            if len(slide_times) % 10 == 0:\n                avg_time = sum(slide_times[-10:]) / 10\n                start_idx = len(slide_times) - 9\n                end_idx = len(slide_times)\n                print(f\" Average time for slides {start_idx}-{end_idx}: {avg_time:.2f} seconds\")\n\n    \n            # save intermediate combined .pt file every 100 slides\n            if idx % 100 == 0:\n                dict_save_path = combined_vectors_dir / f\"slide_vectors_{idx}_{encoder_combo}.pt\"\n                torch.save(slide_vectors, dict_save_path)\n                print(f\" Saved intermediate {idx} slide vectors to {dict_save_path}\")\n                slide_vectors.clear()  # Clear dictionary to free memory\n\n\n        except Exception as e:\n            print(f\" Error processing {slide_path.name}: {e}\")\n\n    # final save of any remaining slide vectors\n    if slide_vectors:\n        dict_save_path = combined_vectors_dir / f\"slide_vectors_final_{encoder_combo}.pt\"\n        torch.save(slide_vectors, dict_save_path)\n        print(f\" Saved final slide vectors to {dict_save_path}\")\n\n    print(\" Combining all intermediate .pt files into one final .pt file...\")\n    final_slide_vectors = {}\n    for pt_file in sorted(combined_vectors_dir.glob(\"slide_vectors_*.pt\")):\n        chunk = torch.load(pt_file, map_location=\"cpu\")\n        final_slide_vectors.update(chunk)\n    final_pt_path = combined_vectors_dir / f\"slide_vectors_ALL_{encoder_combo}.pt\"\n    torch.save(final_slide_vectors, final_pt_path)\n    print(f\"Final combined .pt file saved at {final_pt_path} with {len(final_slide_vectors)} slides!\")\n\n\n    # final summary\n    if slide_times:\n        total_avg = sum(slide_times) / len(slide_times)\n        print(f\"\\n Extracted {len(slide_paths)} slide vectors to {cfg.output_dir}\")\n        print(f\" Final average processing time per slide: {total_avg:.2f} seconds\")\n    else:\n        print(\" No slides were processed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T11:59:47.590818Z","iopub.execute_input":"2025-05-26T11:59:47.591239Z","iopub.status.idle":"2025-05-26T11:59:47.606340Z","shell.execute_reply.started":"2025-05-26T11:59:47.591209Z","shell.execute_reply":"2025-05-26T11:59:47.605796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # Initial config setup\n    cfg = Config(\n        do_deduplication=False,\n        do_penmark_check=False,\n        preprocessing=True  # this will be adjusted\n    )\n\n    # Disable preprocessing if no checks are requested\n    if not cfg.do_deduplication and not cfg.do_penmark_check:\n        print(\" Neither removing duplicates nor penmarks removal\")\n        cfg.preprocessing = False\n\n    # Run pipeline\n    if cfg.preprocessing:\n        preprocess_slides(cfg)\n    else:\n        print(\" Skip preprocessing\")\n\n    extract_features(cfg)\n\n#  Entry point\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T11:53:43.176396Z","iopub.execute_input":"2025-05-26T11:53:43.176662Z","iopub.status.idle":"2025-05-26T11:55:15.553148Z","shell.execute_reply.started":"2025-05-26T11:53:43.176643Z","shell.execute_reply":"2025-05-26T11:55:15.551886Z"}},"outputs":[],"execution_count":null}]}